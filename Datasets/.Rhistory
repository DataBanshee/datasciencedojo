## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
titanic.model <- randomForest(Survived~., data=titanic.train, importance=TRUE, ntree=500)
titanic.model
## MODEL EVALUATION
## to predict using logistic regression model, probablilities obtained
titanic.test.predictions <- predict(titanic.model, titanic.test, type="response")
## extract out the observation for titanic.testing dataset
titanic.test.observations <- titanic.test[,1]
## show the confusion table
confusion.matrix <- table(titanic.test.predictions, titanic.test.observations)
confusion.matrix
## accuracy
accuracy <- sum(diag(confusion.matrix)) / sum(confusion.matrix)
accuracy
## precision
precision <- confusion.matrix[2,2] / sum(confusion.matrix[2,])
precision
## recall
recall <- confusion.matrix[2,2] / sum(confusion.matrix[,2])
recall
## F1 score
F1.score <- 2 * precision * recall / (precision + recall)
F1.score
## show the importance of variables
importance(titanic.model)
varImpPlot(titanic.model)
